{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import skeletondef as skd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PFNNHiddenLayer import PFNNHiddenLayer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124610, 342) (124610, 311) (124610,)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "X = np.float32(np.loadtxt('./data/Input.txt'))\n",
    "Y = np.float32(np.loadtxt('./data/Output.txt'))\n",
    "P = np.float32(np.loadtxt('./data/Phases.txt'))\n",
    "print(X.shape, Y.shape, P.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean and std\n",
    "Xmean, Xstd = X.mean(axis=0), X.std(axis=0)\n",
    "Ymean, Ystd = Y.mean(axis=0), Y.std(axis=0)\n",
    "\n",
    "j = skd.JOINT_NUM\n",
    "w = ((60*2)//10)\n",
    "\n",
    "Xstd[w*0:w* 1] = Xstd[w*0:w* 1].mean() # Trajectory Past Positions\n",
    "Xstd[w*1:w* 2] = Xstd[w*1:w* 2].mean() # Trajectory Future Positions\n",
    "Xstd[w*2:w* 3] = Xstd[w*2:w* 3].mean() # Trajectory Past Directions\n",
    "Xstd[w*3:w* 4] = Xstd[w*3:w* 4].mean() # Trajectory Future Directions\n",
    "Xstd[w*4:w*10] = Xstd[w*4:w*10].mean() # Trajectory Gait\n",
    "\n",
    "# mask out unused joints in input\n",
    "joint_weights = np.array(skd.JOINT_WEIGHTS).repeat(3)\n",
    "\n",
    "Xstd[w*10+j*3*0:w*10+j*3*1] = Xstd[w*10+j*3*0:w*10+j*3*1].mean() / (joint_weights * 0.1) # Pos\n",
    "Xstd[w*10+j*3*1:w*10+j*3*2] = Xstd[w*10+j*3*1:w*10+j*3*2].mean() / (joint_weights * 0.1) # Vel\n",
    "Xstd[w*10+j*3*2:          ] = Xstd[w*10+j*3*2:          ].mean() # Terrain\n",
    "\n",
    "Ystd[0:2] = Ystd[0:2].mean() # Translational Velocity\n",
    "Ystd[2:3] = Ystd[2:3].mean() # Rotational Velocity\n",
    "Ystd[3:4] = Ystd[3:4].mean() # Change in Phase\n",
    "Ystd[4:8] = Ystd[4:8].mean() # Contacts\n",
    "\n",
    "Ystd[8+w*0:8+w*1] = Ystd[8+w*0:8+w*1].mean() # Trajectory Future Positions\n",
    "Ystd[8+w*1:8+w*2] = Ystd[8+w*1:8+w*2].mean() # Trajectory Future Directions\n",
    "\n",
    "Ystd[8+w*2+j*3*0:8+w*2+j*3*1] = Ystd[8+w*2+j*3*0:8+w*2+j*3*1].mean() # Pos\n",
    "Ystd[8+w*2+j*3*1:8+w*2+j*3*2] = Ystd[8+w*2+j*3*1:8+w*2+j*3*2].mean() # Vel\n",
    "Ystd[8+w*2+j*3*2:8+w*2+j*3*3] = Ystd[8+w*2+j*3*2:8+w*2+j*3*3].mean() # Rot\n",
    "\n",
    "# save mean / std / min / max\n",
    "\n",
    "Xmean.astype(np.float32).tofile('./weights/Xmean.bin')\n",
    "Ymean.astype(np.float32).tofile('./weights/Ymean.bin')\n",
    "Xstd.astype(np.float32).tofile('./weights/Xstd.bin')\n",
    "Ystd.astype(np.float32).tofile('./weights/Ystd.bin')\n",
    "\n",
    "# normalize data\n",
    "X = (X - Xmean) / Xstd\n",
    "Y = (Y - Ymean) / Ystd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhaseFunctionedNetwork(nn.Module):\n",
    "    def __init__(self, input_shape=1, output_shape=1, dropout=0.7):\n",
    "        super(PhaseFunctionedNetwork, self).__init__()\n",
    "\n",
    "        self.nslices = 4\n",
    "        self.dropout0 = nn.Dropout(p=dropout)\n",
    "        self.hidden0 = PFNNHiddenLayer((self.nslices, 512, input_shape-1))\n",
    "        self.activation0 = nn.ELU()\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.hidden1 = PFNNHiddenLayer((self.nslices, 512, 512))\n",
    "        self.activation1 = nn.ELU()\n",
    "        \n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.hidden2 = PFNNHiddenLayer((self.nslices, output_shape, 512))\n",
    "\n",
    "        self.layers = nn.ModuleList([self.hidden0, self.hidden1, self.hidden2])\n",
    "\n",
    "    def forward(self, input):\n",
    "        pscale = self.nslices * input[:, -1]\n",
    "        pamount = pscale % 1.0\n",
    "\n",
    "        pindex_1 = torch.floor(pscale).long() % self.nslices\n",
    "        pindex_0 = (pindex_1 - 1) % self.nslices\n",
    "        pindex_2 = (pindex_1 + 1) % self.nslices\n",
    "        pindex_3 = (pindex_1 + 2) % self.nslices\n",
    "\n",
    "        Wamount = pamount.unsqueeze(1).unsqueeze(1)\n",
    "        bamount = pamount.unsqueeze(1)\n",
    "\n",
    "        def cubic(y0, y1, y2, y3, mu):\n",
    "            return (\n",
    "                (-0.5*y0+1.5*y1-1.5*y2+0.5*y3)*mu*mu*mu + \n",
    "                (y0-2.5*y1+2.0*y2-0.5*y3)*mu*mu + \n",
    "                (-0.5*y0+0.5*y2)*mu +\n",
    "                (y1))\n",
    "        \n",
    "        # interpolate weights and biases for update using cubic Catmull-Rom spline function\n",
    "        W0 = cubic(self.hidden0.W[pindex_0], self.hidden0.W[pindex_1], self.hidden0.W[pindex_2], self.hidden0.W[pindex_3], Wamount)\n",
    "        b0 = cubic(self.hidden0.b[pindex_0], self.hidden0.b[pindex_1], self.hidden0.b[pindex_2], self.hidden0.b[pindex_3], bamount)\n",
    "\n",
    "        W1 = cubic(self.hidden1.W[pindex_0], self.hidden1.W[pindex_1], self.hidden1.W[pindex_2], self.hidden1.W[pindex_3], Wamount)\n",
    "        b1 = cubic(self.hidden1.b[pindex_0], self.hidden1.b[pindex_1], self.hidden1.b[pindex_2], self.hidden1.b[pindex_3], bamount)\n",
    "        \n",
    "        W2 = cubic(self.hidden2.W[pindex_0], self.hidden2.W[pindex_1], self.hidden2.W[pindex_2], self.hidden2.W[pindex_3], Wamount)\n",
    "        b2 = cubic(self.hidden2.b[pindex_0], self.hidden2.b[pindex_1], self.hidden2.b[pindex_2], self.hidden2.b[pindex_3], bamount)\n",
    "\n",
    "        # forward propogation\n",
    "        x = input[:,:-1]\n",
    "        x = self.dropout0(x.unsqueeze(-1))\n",
    "        x = self.activation0(self.hidden0(W0, b0, x))\n",
    "\n",
    "        x = self.dropout1(x.unsqueeze(-1))\n",
    "        x = self.activation1(self.hidden1(W1, b1, x))\n",
    "\n",
    "        x = self.dropout2(x.unsqueeze(-1))\n",
    "        x = self.hidden2(W2, b2, x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def cost(self):\n",
    "        costs = 0\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'cost'):\n",
    "                costs += layer.cost()\n",
    "        return costs / len(self.layers)\n",
    "    \n",
    "    def precompute_and_save_weights(self):\n",
    "        # precompute weights\n",
    "        for i in range(50):\n",
    "    \n",
    "            pscale = self.nslices*(float(i)/50)\n",
    "            pamount = pscale % 1.0\n",
    "            \n",
    "            pindex_1 = int(pscale) % self.nslices\n",
    "            pindex_0 = (pindex_1-1) % self.nslices\n",
    "            pindex_2 = (pindex_1+1) % self.nslices\n",
    "            pindex_3 = (pindex_1+2) % self.nslices\n",
    "            \n",
    "            def cubic(y0, y1, y2, y3, mu):\n",
    "                return (\n",
    "                    (-0.5*y0+1.5*y1-1.5*y2+0.5*y3)*mu*mu*mu + \n",
    "                    (y0-2.5*y1+2.0*y2-0.5*y3)*mu*mu + \n",
    "                    (-0.5*y0+0.5*y2)*mu +\n",
    "                    (y1))\n",
    "            \n",
    "            # interpolate weights and biases\n",
    "            W0 = cubic(self.hidden0.W[pindex_0], self.hidden0.W[pindex_1], self.hidden0.W[pindex_2], self.hidden0.W[pindex_3], pamount).cpu().detach().numpy()\n",
    "            b0 = cubic(self.hidden0.b[pindex_0], self.hidden0.b[pindex_1], self.hidden0.b[pindex_2], self.hidden0.b[pindex_3], pamount).cpu().detach().numpy()\n",
    "\n",
    "            W1 = cubic(self.hidden1.W[pindex_0], self.hidden1.W[pindex_1], self.hidden1.W[pindex_2], self.hidden1.W[pindex_3], pamount).cpu().detach().numpy()\n",
    "            b1 = cubic(self.hidden1.b[pindex_0], self.hidden1.b[pindex_1], self.hidden1.b[pindex_2], self.hidden1.b[pindex_3], pamount).cpu().detach().numpy()\n",
    "\n",
    "            W2 = cubic(self.hidden2.W[pindex_0], self.hidden2.W[pindex_1], self.hidden2.W[pindex_2], self.hidden2.W[pindex_3], pamount).cpu().detach().numpy()\n",
    "            b2 = cubic(self.hidden2.b[pindex_0], self.hidden2.b[pindex_1], self.hidden2.b[pindex_2], self.hidden2.b[pindex_3], pamount).cpu().detach().numpy()\n",
    "            \n",
    "            # save precomputed weights and biases\n",
    "            W0.astype(np.float32).tofile('./weights/W0_%03i.bin' % i)\n",
    "            b0.astype(np.float32).tofile('./weights/b0_%03i.bin' % i)\n",
    "\n",
    "            W1.astype(np.float32).tofile('./weights/W1_%03i.bin' % i)\n",
    "            b1.astype(np.float32).tofile('./weights/b1_%03i.bin' % i)\n",
    "            \n",
    "            W2.astype(np.float32).tofile('./weights/W2_%03i.bin' % i)\n",
    "            b2.astype(np.float32).tofile('./weights/b2_%03i.bin' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append phase as additional feature\n",
    "input = torch.tensor(np.concatenate([X, P [..., np.newaxis]], axis=-1))\n",
    "target = torch.tensor(Y)\n",
    "\n",
    "dataset = TensorDataset(input[:100000], target[:100000])\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure GPU is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "device = \"cuda\"\n",
    "\n",
    "# custom loss function\n",
    "def loss_func(output, target, model):\n",
    "    loss = torch.mean((output - target)**2) + model.cost()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 24/3125 [00:16<35:39,  1.45it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     24\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 25\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39maverage(loss_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ana\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ana\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Ana\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\adamw.py:187\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    174\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    176\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    177\u001b[0m         group,\n\u001b[0;32m    178\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    184\u001b[0m         state_steps,\n\u001b[0;32m    185\u001b[0m     )\n\u001b[1;32m--> 187\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Ana\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\adamw.py:339\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    337\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 339\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ana\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\adamw.py:472\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 472\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = PhaseFunctionedNetwork(input_shape=input.shape[1], output_shape=311)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs=2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        input, target = batch\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        output = model(input)\n",
    "        loss = loss_func(output, target, model)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {np.average(loss_list)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "model.precompute_and_save_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
